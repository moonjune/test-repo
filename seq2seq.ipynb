{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moonjune/test-repo/blob/master/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4ITPlCx_tL6",
        "colab_type": "code",
        "outputId": "32eff8a4-763f-4f8d-eb1c-93a9969d53e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "!pip install tensorflow==1.12"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.12 in /usr/local/lib/python3.6/dist-packages (1.12.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (3.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.7)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.16.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n",
            "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.12.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (0.15.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYH6xM0sDB6C",
        "colab_type": "code",
        "outputId": "be6c46cc-0008-45bf-c02e-8d285da4f393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "# 임포트 된 것을 파악하여 \n",
        "import tensorflow as tf\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "from matplotlib import font_manager, rc\n",
        "\n",
        "rc('font', family='AppleGothic') #for mac\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MNxIpfHDHux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sources = [['I', 'feel', 'hungry'],\n",
        "     ['tensorflow', 'is', 'very', 'difficult'],\n",
        "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
        "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
        "# 4개 케이스, 소스\n",
        "targets = [['나는', '배가', '고프다'],\n",
        "           ['텐서플로우는', '매우', '어렵다'],\n",
        "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
        "           ['텐서플로우는', '매우', '빠르게', '변화한다']]\n",
        "# 4개 케이스, 타겟"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2g1CZk8DOUx",
        "colab_type": "code",
        "outputId": "0afd3c08-ee9b-48fe-9d78-7282fd362eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# vocabulary for sources\n",
        "s_vocab = list(set(sum(sources, [])))\n",
        "\n",
        "# s_vocab은 왜 정의하는가? 엠베딩 하기 위해 단어별를 원소로 공간을 형성\n",
        "# sum은 문자열을 합치고 있다. 조인 함수의 경우 어떻게 되는가? 안됨\n",
        "# 차원이 늘어나면 join으로 합칠 수 없나 봄. 그래서 sum을 사용하게 됨. 뒤의 []는 뭐지...; 변수의 type과 맞춰줘야 하나 봄\n",
        "# set은 중복 제거, 고유 원소로만 만드나 봄. 얘는 형태가 set 타입으로 변경됨\n",
        "# 그리고 다시 이걸 list로.. 형태를 복구시켜 줌. unique함수는 numpy나 pandas 쪽이어서 list엔 없음\n",
        "# 핵심은 문장의 형태인 단어 조합을 해체하여 단어별로 취급하고 중복을 제거하는 것 \n",
        "\n",
        "s_vocab.sort()\n",
        "s_vocab = ['<pad>'] + s_vocab\n",
        "# 거기에 패딩 원소 추가\n",
        "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
        "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n",
        "# 넘버링 딕셔너리\n",
        "\n",
        "pprint(source2idx)\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<pad>': 0,\n",
            " 'I': 1,\n",
            " 'a': 2,\n",
            " 'changing': 3,\n",
            " 'deep': 4,\n",
            " 'difficult': 5,\n",
            " 'fast': 6,\n",
            " 'feel': 7,\n",
            " 'for': 8,\n",
            " 'framework': 9,\n",
            " 'hungry': 10,\n",
            " 'is': 11,\n",
            " 'learning': 12,\n",
            " 'tensorflow': 13,\n",
            " 'very': 14}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nXdxBIpFaoD",
        "colab_type": "code",
        "outputId": "f40d1018-7b49-4f8b-aa6e-17e6363ff843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "# vocabularu for targets\n",
        "t_vocab = list(set(sum(targets,[])))\n",
        "t_vocab.sort()\n",
        "t_vocab = ['<pad>', '<bos>','<eos>'] + t_vocab\n",
        "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
        "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n",
        "#타겟도 마찬가지 처리\n",
        "\n",
        "pprint(target2idx)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<bos>': 1,\n",
            " '<eos>': 2,\n",
            " '<pad>': 0,\n",
            " '고프다': 3,\n",
            " '나는': 4,\n",
            " '딥러닝을': 5,\n",
            " '매우': 6,\n",
            " '배가': 7,\n",
            " '변화한다': 8,\n",
            " '빠르게': 9,\n",
            " '어렵다': 10,\n",
            " '위한': 11,\n",
            " '텐서플로우는': 12,\n",
            " '프레임워크이다': 13}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeTEKu9fJ0zS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#전처리 함수 정의\n",
        "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
        "  assert mode in ['source','target'], 'source와 target 중에 선택해주세요'\n",
        "  # 뻑내면 나올 오류\n",
        "  \n",
        "  if mode == 'source': \n",
        "    # 소스 처리를 위한 셀(인코더)\n",
        "    s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
        "    # 받는 것은 source로 문장 리스트를 받았음. \n",
        "    # 그런데 map 함수를 이용하면 리스트 내의 모든 하위 원소에 반응하는 것 같음. [문장1]이 아닌 ['단어1','단어2'] 등\n",
        "    # dic은 뒤에서 source2idx 인덱스로 들어가는 것\n",
        "    # 이 과정을 거쳐 s_input은 1차 embedding 완료\n",
        "    s_len = list(map(lambda sentence : len(sentence), s_input))\n",
        "    # s_input의 형태는 [['단어1_인덱스','단어2_인덱스'],[단어1인덱스, ..]]이럴건데..\n",
        "    # 위와 차이는 for 문이 없다는 것이다. \n",
        "    # lambda 자체가 인스턴스 함수로 애시당초 시퀀스를 변수로 받는다. 위의 식은 거기에 한번 더 for를 씌워줘서 하윗단까지 가게 한거\n",
        "    # 여기서는 투입 리스트 원소만 처리하면 되니 for 문을 쓰지 않았고 s_len은 투입받은 리스트의 원소(문장)의 길이를 세 준 것\n",
        "    s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
        "    # 다시 s_input을 조정한다. pad_sequences는 패딩 씌우는 것\n",
        "    # max_len에 맞게 10개의 길이로 통일시키기 위해 10개 이하는 뒤에 0(<pad>)을 붙여주고 넘치면 뒤를 잘라버림\n",
        "    return s_len, s_input\n",
        "    # 결과로 s_len과 s_input이 출력됨\n",
        "    \n",
        "  elif mode == 'target':\n",
        "    # 타겟 처리를 위한 셀(디코더)\n",
        "    # 투입물\n",
        "    t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n",
        "    # 앞에 <bos>가 붙어서 문장의 시작을 알려서 y1이 문장의 첫 단어가 되도록 해준다. \n",
        "    # 뒤에 <eos>로 문장이 종료되었음을 선언\n",
        "    # 이것은 번역이나 대화를 위한 세팅. 디코더가 만들 문장은 인코더 셀을 다 지난 후 bos로 시작하고 eos로 종료시킴\n",
        "    t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
        "    # s_input과 비슷하다. <bos>,<eos>가 추가된다.    \n",
        "    t_len = list(map(lambda sentence : len(sentence), t_input))\n",
        "    t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
        "    # <bos>,<eos>가 추가되었지만 maxlen은 동일함. 이건 나중에 살펴보자\n",
        "    # target은 번역되는 대상\n",
        "    \n",
        "    # output\n",
        "    t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n",
        "    # 타겟 아웃풋은 먼저 문장단위에 ['eos]만 붙여 투입한다.\n",
        "    t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
        "    # target2idx를 쓰지만 역시 숫자로 이뤄진 결과를 도출한다.\n",
        "    t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
        "    \n",
        "    return t_len, t_input, t_output\n",
        "  \n",
        "  #전처리는 엠베딩 과정, 그 과정에서 <pad>등을 포함한 엠베딩을 만들고 입력 문자를 형식에 맞춰 뱉어내도록 하는 것\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLcg85C2CAeh",
        "colab_type": "code",
        "outputId": "4e9147c4-af6e-45b7-e935-ff3d130a8bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# 소스 전처리(위의 정의 함수)\n",
        "s_max_len = 10\n",
        "s_len, s_input =preprocess(sequences = sources,\n",
        "                           max_len = s_max_len, dic = source2idx, mode = 'source')\n",
        "print(s_len, s_input)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 4, 7, 5] [[ 1  7 10  0  0  0  0  0  0  0]\n",
            " [13 11 14  5  0  0  0  0  0  0]\n",
            " [13 11  2  9  8  4 12  0  0  0]\n",
            " [13 11 14  6  3  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0jTzQZjCdJ5",
        "colab_type": "code",
        "outputId": "bbb23d51-8e76-47f1-bb22-3cf38bb9014a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "# 타겟 전처림\n",
        "t_max_len = 12\n",
        "#타겟의 길이는 12. 아마도 패딩들을 고려한 것으로 보임\n",
        "t_len, t_input, t_output = preprocess(sequences = targets,\n",
        "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n",
        "print(t_len, t_input, t_output)\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 5, 6, 6] [[ 1  4  7  3  2  0  0  0  0  0  0  0]\n",
            " [ 1 12  6 10  2  0  0  0  0  0  0  0]\n",
            " [ 1 12  5 11 13  2  0  0  0  0  0  0]\n",
            " [ 1 12  6  9  8  2  0  0  0  0  0  0]] [[ 4  7  3  2  0  0  0  0  0  0  0  0]\n",
            " [12  6 10  2  0  0  0  0  0  0  0  0]\n",
            " [12  5 11 13  2  0  0  0  0  0  0  0]\n",
            " [12  6  9  8  2  0  0  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP_DeeyMD7Sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 하이퍼 파라미터\n",
        "epochs = 100 # 몇번 돌릴 건가\n",
        "batch_size = 4 # 한번에 장전할 문장 수(전체가 4개니..)\n",
        "learning_rate = .005 # 학습률이 학습률이지 뭐..\n",
        "total_step = epochs / batch_size \n",
        "# 이게 뭐지 대체;;; 의미적으로는 전체 배치가 도는 스텝인거고 epoch는 한 문장의 돌리는 의미인가?\n",
        "buffer_size = 100 # 얘는 아직 모르겠음 ㅎㅎ\n",
        "n_batch = buffer_size //batch_size # 배치의 개수는 버퍼 크기 나누기 배치 사이즈..\n",
        "embedding_dim = 32 # 얘는 왜 고정된 \n",
        "units = 32 # hidden size인듯\n",
        "\n",
        "#input\n",
        "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
        "#데이터를 메모리에 넣어주는 기능. 0번째 차원의 크기가 동일해야 함(케이스의 수 정도)\n",
        "data = data.shuffle(buffer_size = buffer_size)\n",
        "# buffer 크기 기준으로 섞기 시작, \n",
        "data = data.batch(batch_size = batch_size)\n",
        "#s_mb_len, s_mb_input, t_mb_len, t_mb_input, t_mb_output = iterator.get_next()\n",
        "# batch size 대로 데이터 준비\n",
        "\n",
        "# 머신러닝 타입 model 에 데이터를 넣기 위해 직관적인거 같다. 텐서들 한번에 넣어주고, 각 텐서들을 동일한 기준으로 섞고, 배치 준비"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80WRLuAN2Lw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gru(units):\n",
        "  # GPU가 있다면 CuDNNGRU 사용 추천(3배 빠름)\n",
        "  # 이 코드는 자동적으로 사용해줍니다.\n",
        "  # GRU 는 gate recurrent unit의 약자. GPU를 사용한다면 여기서 사용하는 것으로 보인다. \n",
        "  if tf.test.is_gpu_available():\n",
        "    return tf.keras.layers.CuDNNGRU(units,\n",
        "                                    return_sequences = True,\n",
        "                                    return_state = True,\n",
        "                                    recurrent_initializer = 'glorot_uniform')\n",
        "  else:\n",
        "    return tf.keras.layers.GRU(units, # 양의 정수로 아웃풋 공간의 차원을 의미\n",
        "                               return_sequences = True, # 이항변수이며, 마지막 sequence만 할지, 아니면 전체 시퀀스를 반환할지 \n",
        "                               return_state = True, # 결과물에 최종 스테이트를 반환할지\n",
        "                               recurrent_activation = 'sigmoid', # rnn의 활성화 함수. 여기선 시그모이드 함수로\n",
        "                               recurrent_initializer = 'glorot_uniform') # 가중치 행렬을 초기화하는 방법, \n",
        "  \n",
        "  # rnn의 게이트를 담당.. 상세한건 나중에"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOqbPG-k4lZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model): # 인코딩(정보를 받고 투입된 정보에 대한 어떤 context를 생성하는 클래스)\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()  \n",
        "    # super는 기본적으로 부모의 함수를 가져오는 것이라고 한다.\n",
        "    # Encoder가 부모로 삼고 있는 클래스는 tf.keras.Model이다. \n",
        "    # super(Encoder,self)는 그럼 뭘 의미하나... 일단은 넘어갈까;; \n",
        "    self.batch_sz = batch_sz # 1회당 넣을 배치 크기\n",
        "    self.enc_units = enc_units # 인코더 유닛 수(32)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # 엠베딩 레이어에 들어갈 단어 열 크기(모름),\n",
        "    # 그런데 말입니다. embedding이란 무엇일까요?\n",
        "    # 일단 keras.layers.embedding 이란 레이어층을 말한다\n",
        "    self.gru = gru(self.enc_units) #gru는 앞서 정의한 gru이고 인코더 유닛 32개 배치\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x) # 인코더 셀은 앞의 변수와 x, hidden을 받는다. x는 엠베딩 레이어에 x가 들어간 것? \n",
        "    # 이 의미는 embedding layer에 x를 통과시키는 것이라고 한다. \n",
        "    # 분명 각 rnn은 embedding 층, 신경망층이 합쳐져 있으므로 납득은 감\n",
        "    output, state = self.gru(x, initial_state = hidden) # 아웃풋과 state는 gru에서 받는 최종 컨텍스트 값과 state 값을 받음\n",
        "    \n",
        "    return output, state\n",
        "    # 그걸 아웃풋으로 내놓음\n",
        "    # call은 다른 의미를 이미 가진 함수로 보임. \n",
        "    # 미리 소환된 class Encoder는 초기화 값을 가지며 소환된다. 다른 명령 없이 바로 (x)를 넣으면 발동.\n",
        " \n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units)) \n",
        "  # 히든 스테이트를 초기화, 히든 스테이트는 배치 사이즈를 행으로, 인코더 유닛수를 열로 하는 0행렬\n",
        "  # 여기서 의미하는 배치 사이즈는 뭘까.. \n",
        "  \n",
        "  # 인코더는 단어 사이즈(인덱스화 한 사전의 길이), 엠베딩 차원(32 지정), units(32 지정), batch_size(4개 지정)을 초기값으로 받음\n",
        "  # 받은 초기값을 기반으로 gru셀과 히든 스테이트를 준비한다.\n",
        "  # 그리고 만들어진 인코더 클래스+(x)를 넣으면 할동 후 output과 state를 출력해 줌"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOwiPXAl6NMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = gru(self.dec_units)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    # 디코더 역시 비슷비슷\n",
        "    # 그러나 fully connected 초기화가 있음\n",
        "    # \n",
        "    \n",
        "  def call(self, x, hidden, enc_output):\n",
        "    \n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    \n",
        "    # output shape는 배치 사이즈에 *1, 히든 사이즈(32)...?\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    \n",
        "    x = self.fc(output)\n",
        "    \n",
        "    return x, state\n",
        "  \n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.dec_units))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7chqReKDHdOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n",
        "decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = 1 - np.equal(real, 0)\n",
        "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = real, logits = pred) * mask\n",
        "  \n",
        "  #print(\"real: {}\".format(real))\n",
        "  #print(\"pred: {}\".format(pred))\n",
        "  #print(\"mask: {}\".format(mast))\n",
        "  #print(\"loss: {}\".format(tf.reduce_mean(loss)))\n",
        "  \n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "# creating optimizer\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# creating chech point (Object-based saving)\n",
        "checkpoint_dir = '.data_out/traing_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(optimizer = optimizer,\n",
        "                                 encoder = encoder,\n",
        "                                 decoder = decoder)\n",
        "\n",
        "# create writer for tensorboard\n",
        "summary_writer = tf.contrib.summary.create_file_writer(logdir = checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ECWDHG_mDs_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "7395c661-899d-49f2-d224-352da27919ee"
      },
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  \n",
        "  for i , (s_len, s_input, t_len, t_input, t_output) in enumerate(data):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "      enc_output, enc_hidden = encoder(s_input, hidden)\n",
        "      \n",
        "      dec_hidden = hidden\n",
        "      \n",
        "      dec_input = tf.expand_dims([target2idx['<bos>']] * batch_size, 1)\n",
        "      \n",
        "      # Teacher Forcing: feeding the ratget as the next input\n",
        "      for t in range(1, t_input.shape[1]):\n",
        "        \n",
        "        predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "        \n",
        "        loss += loss_function(t_input[:, t], predictions)\n",
        "        \n",
        "        dec_input = tf.expand_dims(t_input[:, t], 1) # using teacher forcing?\n",
        "        \n",
        "    batch_loss = (loss / int(t_input.shape[1]))\n",
        "    \n",
        "    total_loss += batch_loss\n",
        "    \n",
        "    variables = encoder.variables + decoder.variables\n",
        "    \n",
        "    gradient = tape.gradient(loss, variables)\n",
        "    \n",
        "    optimizer.apply_gradients(zip(gradient, variables))\n",
        "    \n",
        "  if epoch % 10 == 0:\n",
        "    #save model every 10 epoch\n",
        "    print('Epoch{} || Loss {:.4f} || Batch Loss {:.4f}'.format(epoch,\n",
        "                                                               total_loss / n_batch,\n",
        "                                                              batch_loss.numpy()))\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "   "
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch0 || Loss 0.0229 || Batch Loss 0.5722\n",
            "Epoch10 || Loss 0.0196 || Batch Loss 0.4899\n",
            "Epoch20 || Loss 0.0161 || Batch Loss 0.4033\n",
            "Epoch30 || Loss 0.0130 || Batch Loss 0.3250\n",
            "Epoch40 || Loss 0.0106 || Batch Loss 0.2646\n",
            "Epoch50 || Loss 0.0088 || Batch Loss 0.2204\n",
            "Epoch60 || Loss 0.0076 || Batch Loss 0.1898\n",
            "Epoch70 || Loss 0.0068 || Batch Loss 0.1696\n",
            "Epoch80 || Loss 0.0062 || Batch Loss 0.1562\n",
            "Epoch90 || Loss 0.0059 || Batch Loss 0.1472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHywi32tscxz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3ae16b3f-4362-4609-9426-0e68339a13b2"
      },
      "source": [
        "# resore checkpoint\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7fe848ed69b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1o5t2mTt9JX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = 'I feel hungry'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY92YSrLuatZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "cff13867-1249-457b-ee4e-2de97e8cf881"
      },
      "source": [
        "def prediction(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "  \n",
        "  inputs = [inp_lang[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen = max_length_inp, padding = 'post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  \n",
        "  result = ''\n",
        "  \n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "  \n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang['<bos>']], 0)\n",
        "  \n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n",
        "    \n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    \n",
        "    result += idx2target[predicted_id] + ' '\n",
        "    \n",
        "    if idx2target.get(predicted_id) == '<eos>':\n",
        "      return result, sentence\n",
        "    \n",
        "    #the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "  return result, sentence\n",
        "\n",
        "result, output_sentence = prediction(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len)\n",
        "\n",
        "print(sentence)\n",
        "print(result)\n",
        "      "
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I feel hungry\n",
            "텐서플로우는 매우 빠르게 변화한다 <eos> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW6ksfPuTXPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}