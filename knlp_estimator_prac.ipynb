{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moonjune/test-repo/blob/master/knlp_estimator_prac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIQxzzsqf3F8",
        "colab_type": "code",
        "outputId": "bb6add2f-b23a-4bc0-98ad-0de428f27f36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "!git clone https://github.com/NLP-kr/tensorflow-ml-nlp.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tensorflow-ml-nlp'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/64)   \u001b[K\rremote: Counting objects:   3% (2/64)   \u001b[K\rremote: Counting objects:   4% (3/64)   \u001b[K\rremote: Counting objects:   6% (4/64)   \u001b[K\rremote: Counting objects:   7% (5/64)   \u001b[K\rremote: Counting objects:   9% (6/64)   \u001b[K\rremote: Counting objects:  10% (7/64)   \u001b[K\rremote: Counting objects:  12% (8/64)   \u001b[K\rremote: Counting objects:  14% (9/64)   \u001b[K\rremote: Counting objects:  15% (10/64)   \u001b[K\rremote: Counting objects:  17% (11/64)   \u001b[K\rremote: Counting objects:  18% (12/64)   \u001b[K\rremote: Counting objects:  20% (13/64)   \u001b[K\rremote: Counting objects:  21% (14/64)   \u001b[K\rremote: Counting objects:  23% (15/64)   \u001b[K\rremote: Counting objects:  25% (16/64)   \u001b[K\rremote: Counting objects:  26% (17/64)   \u001b[K\rremote: Counting objects:  28% (18/64)   \u001b[K\rremote: Counting objects:  29% (19/64)   \u001b[K\rremote: Counting objects:  31% (20/64)   \u001b[K\rremote: Counting objects:  32% (21/64)   \u001b[K\rremote: Counting objects:  34% (22/64)   \u001b[K\rremote: Counting objects:  35% (23/64)   \u001b[K\rremote: Counting objects:  37% (24/64)   \u001b[K\rremote: Counting objects:  39% (25/64)   \u001b[K\rremote: Counting objects:  40% (26/64)   \u001b[K\rremote: Counting objects:  42% (27/64)   \u001b[K\rremote: Counting objects:  43% (28/64)   \u001b[K\rremote: Counting objects:  45% (29/64)   \u001b[K\rremote: Counting objects:  46% (30/64)   \u001b[K\rremote: Counting objects:  48% (31/64)   \u001b[K\rremote: Counting objects:  50% (32/64)   \u001b[K\rremote: Counting objects:  51% (33/64)   \u001b[K\rremote: Counting objects:  53% (34/64)   \u001b[K\rremote: Counting objects:  54% (35/64)   \u001b[K\rremote: Counting objects:  56% (36/64)   \u001b[K\rremote: Counting objects:  57% (37/64)   \u001b[K\rremote: Counting objects:  59% (38/64)   \u001b[K\rremote: Counting objects:  60% (39/64)   \u001b[K\rremote: Counting objects:  62% (40/64)   \u001b[K\rremote: Counting objects:  64% (41/64)   \u001b[K\rremote: Counting objects:  65% (42/64)   \u001b[K\rremote: Counting objects:  67% (43/64)   \u001b[K\rremote: Counting objects:  68% (44/64)   \u001b[K\rremote: Counting objects:  70% (45/64)   \u001b[K\rremote: Counting objects:  71% (46/64)   \u001b[K\rremote: Counting objects:  73% (47/64)   \u001b[K\rremote: Counting objects:  75% (48/64)   \u001b[K\rremote: Counting objects:  76% (49/64)   \u001b[K\rremote: Counting objects:  78% (50/64)   \u001b[K\rremote: Counting objects:  79% (51/64)   \u001b[K\rremote: Counting objects:  81% (52/64)   \u001b[K\rremote: Counting objects:  82% (53/64)   \u001b[K\rremote: Counting objects:  84% (54/64)   \u001b[K\rremote: Counting objects:  85% (55/64)   \u001b[K\rremote: Counting objects:  87% (56/64)   \u001b[K\rremote: Counting objects:  89% (57/64)   \u001b[K\rremote: Counting objects:  90% (58/64)   \u001b[K\rremote: Counting objects:  92% (59/64)   \u001b[K\rremote: Counting objects:  93% (60/64)   \u001b[K\rremote: Counting objects:  95% (61/64)   \u001b[K\rremote: Counting objects:  96% (62/64)   \u001b[K\rremote: Counting objects:  98% (63/64)   \u001b[K\rremote: Counting objects: 100% (64/64)   \u001b[K\rremote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 842 (delta 30), reused 27 (delta 15), pack-reused 778\u001b[K\n",
            "Receiving objects: 100% (842/842), 160.14 MiB | 26.08 MiB/s, done.\n",
            "Resolving deltas: 100% (497/497), done.\n",
            "Checking out files: 100% (100/100), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iheum3B7f4FH",
        "colab_type": "code",
        "outputId": "92e7aab9-c7fc-406b-d1dc-9c34767cbe87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1234
        }
      },
      "source": [
        "import os\n",
        "os.chdir('/content/tensorflow-ml-nlp')\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow>=1.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.24.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.0.3)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.0.1)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (3.2.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (3.6.0)\n",
            "Collecting konlpy (from -r requirements.txt (line 11))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/3d/4e983cd98d87b50b2ab0387d73fa946f745aa8164e8888a714d5129f9765/konlpy-0.5.1-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (0.90)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (4.28.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.0.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.16.3)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.0.9)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.13.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->-r requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->-r requirements.txt (line 5)) (4.6.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->-r requirements.txt (line 6)) (0.5.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud->-r requirements.txt (line 7)) (4.3.0)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r requirements.txt (line 10)) (1.8.3)\n",
            "Collecting JPype1>=0.5.7 (from konlpy->-r requirements.txt (line 11))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/4b/60a3e63d51714d4d7ef1b1efdf84315d118a0a80a5b085bb52a7e2428cdc/JPype1-0.6.3.tar.gz (168kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 45.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow>=1.10->-r requirements.txt (line 1)) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.10->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.10->-r requirements.txt (line 1)) (0.15.4)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow>=1.10->-r requirements.txt (line 1)) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow>=1.10->-r requirements.txt (line 1)) (41.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 2)) (0.12.5)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->wordcloud->-r requirements.txt (line 7)) (0.46)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (1.9.153)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (2.8)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.153 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (1.12.153)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.153->boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (0.14)\n",
            "Building wheels for collected packages: JPype1\n",
            "  Building wheel for JPype1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/2b/e8/c0b818ac4b3d35104d35e48cdc7afe27fc06ea277feed2831a\n",
            "Successfully built JPype1\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-0.6.3 konlpy-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZc2NU3sf73H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode, params, config):\n",
        "  \n",
        "  # 모델 구현 부분\n",
        "  estimator = tf.estimator.Estimator(model_fn = model_fn, model_dir = ...,\n",
        "                                     configs = ..., params = ...)\n",
        "  \n",
        "  return tf.estimator.EstimatrSpec( ... )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF7wqrhYncNI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "aa331400-0f35-4b33-c321-435ac4163a12"
      },
      "source": [
        "\"\"\"\n",
        "모델을 학습하는 명령어\n",
        "estimator.train(input_fn = ...)\n",
        "\n",
        "학습한 모델을 검증\n",
        "estimator.evaluate(input_fn = ...)\n",
        "\n",
        "학습한 모델을 통한 예측\n",
        "estimator.predict(input_fn = ...)\n",
        "\"\"\"\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n모델을 학습하는 명령어\\nestimator.train(input_fn = ...)\\n\\n학습한 모델을 검증\\nestimator.evaluate(input_fn = ...)\\n\\n학습한 모델을 통한 예측\\nestimator.predict(input_fn = ...)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne7lWGi8n_tM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_input_fn():\n",
        "  \n",
        "  # 데이터 파이프라인 구현 부분\n",
        "  \n",
        "  return features, labels\n",
        "\n",
        "# tf.data에서 사용한 셔플 배치, 반복 등의 기능 이용 예정\n",
        "# 학습, 검증, 예측 등 상황에 따라 필요 기능이 다르므로 각 기능에 따라 따로 구현"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXSvmEiZzrne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#패키지와 데이터\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "samples = ['너 오늘 이뻐 보인다', \n",
        "           '나는 오늘 기분이 더러워', \n",
        "           '끝내주는데, 좋은 일이 있나봐', \n",
        "           '나 좋은 일이 생겼어', \n",
        "           '아 오늘 진짜 짜증나', \n",
        "           '환상적인데, 정말 좋은거 같아']\n",
        "\n",
        "labels = [[1], [0], [1], [1], [0], [1]]\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(samples)\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1BbUFbr0iFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 파이프라인\n",
        "EPOCH = 100\n",
        "\n",
        "def train_input_fn():\n",
        "  \n",
        "  dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
        "  dataset = dataset.repeat(EPOCH)\n",
        "  dataset = dataset.batch(1)\n",
        "  dataset = dataset.shuffle(len(sequences))\n",
        "  iterator = dataset.make_one_shot_iterator()\n",
        "  \n",
        "  return iterator.get_next()  \n",
        "\n",
        "# 이 함수가 뱉는 것은 실행하면 다음 데이터 행을 뱉는 객체이다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTkeUzvq3d6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 에스티메이터\n",
        "\n",
        "VOCAB_SIZE = len(word_index) + 1  # word_index는 딕셔너리로 띄어쓰기로 구분된 모든 단어들(중복제외)의 집합이며, 그 길이가 dlrjt\n",
        "EMB_SIZE = 128 # EMB_SIZE는 에.. 128개로 고정... 이따 보자\n",
        "\n",
        "def model_fn(features, labels, mode):\n",
        "  \n",
        "  TRAIN = mode == tf.estimator.ModeKeys.TRAIN # loss, train_op필요\n",
        "  EVAL = mode == tf.estimator.ModeKeys.EVAL # loss 필요\n",
        "  PREDICT = mode == tf.estimator.ModeKeys.PREDICT # prediction 필요\n",
        "  # 각 모드에 따라 \n",
        "  \n",
        "  embed_input = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features) \n",
        "  embed_input = tf.reduce_mean(embed_input, axis = 1)\n",
        "  # embedding은 텍스트의 수치화, 존재하는 텍스트 길이와 128개 요인으로 만드는 듯\n",
        "  # embedding layer는 128개에서 발생한 수치를 평균화하여 원래 VOCAB_SIZE로 만든다.\n",
        "  # 가 아니네;;; vocab_size를 평균화해버리고 요인 128개 차원의 데이터로 만들어 줌;\n",
        "  # 얘는 아직 기본 DNN이고 RNN이 아니다 모든 단어를 128개 요인의 수치로 표현된 다음 그걸 요인별로 평균화 함\n",
        "  # 아직 word2vec 학습이 되거나 한건 아니다. 추후 얘의 정체 알 필요\n",
        "  \n",
        "  hidden_layer = tf.keras.layers.Dense(128, activation = tf.nn.relu)(embed_input)\n",
        "  output_layer = tf.keras.layers.Dense(1)(hidden_layer)\n",
        "  output = tf.nn.sigmoid(output_layer)\n",
        "  # 128개의 노드로 받아서 함. 렐루로 값을 보냄\n",
        "  # dense_layer_2는 정의된 적이 없음.. 함수 밖에서 가져올 수 있나?\n",
        "  \n",
        "  loss = tf.losses.mean_squared_error(output, labels)\n",
        "  # 지도학습이므로 차이를 mean squared error로 계산\n",
        "  \n",
        "  if TRAIN:\n",
        "    global_step = tf.train.get_global_step()\n",
        "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
        "    # global_step은 backprop을 하기 위한 것으로 보임\n",
        "    # train.op는 옵티마이저이고 loss값을 global_step으로 미분???\n",
        "    # global step은 옵션값으로 미니마이즈 될 변수가 1회 업데이트 된 후 1 증가하는 것으로 step 함수\n",
        "    \n",
        "    return tf.estimator.EstimatorSpec(mode = mode,\n",
        "                                     train_op = train_op,\n",
        "                                     loss = loss)\n",
        "  \n",
        "  # estimator의 스펙을 결정, train이니 train_op와 loss가 필요"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZujRAoVXFfyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "outputId": "dd440e34-541a-4a29-90e1-ccfa10ed3e74"
      },
      "source": [
        "# 실제 학습\n",
        "DATA_OUT_PATH = './data_out/'\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists(DATA_OUT_PATH):\n",
        "  os.makedirs(DATA_OUT_PATH)\n",
        "  \n",
        "estimator = tf.estimator.Estimator(model_fn = model_fn, model_dir = DATA_OUT_PATH + 'checkpoint/dnn')\n",
        "estimator.train(train_input_fn)\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': './data_out/checkpoint/dnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4c138c07b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into ./data_out/checkpoint/dnn/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.24911827, step = 1\n",
            "INFO:tensorflow:global_step/sec: 705.695\n",
            "INFO:tensorflow:loss = 0.0043774247, step = 101 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 999.02\n",
            "INFO:tensorflow:loss = 0.0009940446, step = 201 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 999.455\n",
            "INFO:tensorflow:loss = 0.0005079355, step = 301 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1050.37\n",
            "INFO:tensorflow:loss = 6.58715e-05, step = 401 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1028.37\n",
            "INFO:tensorflow:loss = 4.2638392e-05, step = 501 (0.096 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 600 into ./data_out/checkpoint/dnn/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.00012684953.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f4c2c2e90f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIh8ltg-GMtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}