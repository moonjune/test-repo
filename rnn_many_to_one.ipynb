{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_many_to_one.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moonjune/test-repo/blob/master/rnn_many_to_one.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLWSLmW5aLH_",
        "colab_type": "code",
        "outputId": "904a4546-be35-4606-cb69-f239a590c666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "!pip install tensorflow==1.12"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.12 in /usr/local/lib/python3.6/dist-packages (1.12.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.9)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.33.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (3.7.1)\n",
            "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.12.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.16.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFfo5Ki77XXE",
        "colab_type": "code",
        "outputId": "b62c2c61-3b2f-4746-b734-3ce0d84486a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "%matplotlib inline\n",
        "\n",
        "print(tf.__version__)\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFPOfPFB8YrH",
        "colab_type": "code",
        "outputId": "34253d1f-cba6-4d80-ed05-4050697e5993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# 데이터 준비. 실제 쓰이는 단어와 그 단어에 좋다 나쁘다  태깅을 한 것\n",
        "words = ['good', 'bad', 'worse', 'so good']\n",
        "y_data = [1,0,0,1]\n",
        "\n",
        "# 토큰 딕쇼너리를 만든다\n",
        "char_set = ['<pad>'] + sorted(list(set(''.join(words))))\n",
        "# 앞에 패드가 들어가는 이유는 워드 방식에선 <pad>를 한 요소로 사용하기 때문임. \n",
        "# 목적에 맞게 요소들은 추가될 것이며 추후 <seq>, <eos>등 다른 요소들도 추가 될거니 너무 신경쓰지 말 것\n",
        "# 뒤의 것은 워즈 리스트에 있는 요소들을 쌩으로 합치고(join), \n",
        "# 이걸 각 문자를 원소로 하는 집합으로 합치고(set, ',' 구분자는 여기서 생김 + 중복 제거) \n",
        "# 그걸 리스트 화 하고(list), 정렬함(sort)\n",
        "# 결과는 words 안에 있는 단어들을 스펠링과 <pad>를 원소로 하는 리스트 생성\n",
        "\n",
        "#글자 단위로 글자 세트 생성\n",
        "idx2char = {idx : char for idx, char in enumerate(char_set)}\n",
        "# enumerate는 인덱스를 붙인 시퀀스로 생성해 줌 for문과 같이 쓰여서 [인덱스, 원래 원소] 이걸 순차적으로 반복해 줌\n",
        "# :를 써도 되는지는 몰랐는데 딕셔너리 생성 시 활용\n",
        "char2idx = {char : idx for idx, char in enumerate(char_set)}\n",
        "\n",
        "print(char_set)\n",
        "print(idx2char)\n",
        "print(char2idx)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<pad>', ' ', 'a', 'b', 'd', 'e', 'g', 'o', 'r', 's', 'w']\n",
            "{0: '<pad>', 1: ' ', 2: 'a', 3: 'b', 4: 'd', 5: 'e', 6: 'g', 7: 'o', 8: 'r', 9: 's', 10: 'w'}\n",
            "{'<pad>': 0, ' ': 1, 'a': 2, 'b': 3, 'd': 4, 'e': 5, 'g': 6, 'o': 7, 'r': 8, 's': 9, 'w': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW_eNYl_FEhu",
        "colab_type": "code",
        "outputId": "5ed65bd3-7004-4221-939d-b491886127c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# 단어들을 만들어진 캐릭터 벡터 인덱스로 치환함\n",
        "x_data = list(map(lambda word : [char2idx.get(char) for char in word], words))\n",
        "# map 함수는 앞에는 함수, 뒤에는 인자를 받는다. \n",
        "# lambda 함수는 인스턴트 함수를 만들어 준다. 여기서 이용되는 것은 word를 받아서 word에 있는 걸 char로 받아서 전개\n",
        "# 적용되는 함수는 char2idx.get(char)인데, ()안에 인자로 key 값을 넣으면 value를 반환해주는 함수이다.\n",
        "# lambda word는 words의 스펠링 하나하나를 char로 받아 char2idx를 통해 key->value로 치환하고 그걸 리스트로 만든다\n",
        "\n",
        "x_data_len = list(map(lambda word : len(word), x_data))\n",
        "# x_data를 인자(word)로 받아서 각 단어들을 len에다 집어넣은 값 출력\n",
        "\n",
        "print(x_data)\n",
        "print(x_data_len)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6, 7, 7, 4], [3, 2, 4], [10, 7, 8, 9, 5], [9, 7, 1, 6, 7, 7, 4]]\n",
            "[4, 3, 5, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUCaHxE-Utu_",
        "colab_type": "code",
        "outputId": "9d8214a9-5427-4962-ee26-5ecd3932ce7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# \n",
        "max_sequence = 10\n",
        "x_data = pad_sequences(sequences = x_data, maxlen = max_sequence,\n",
        "                      padding = 'post', truncating = 'post')\n",
        "# pad_sequences는 함수로 keras의 전처리를 위한 함수, 기초 인자는 시퀀스 데이터, 최대 시퀀스 길이, 등등..?\n",
        "# padding은 하라는 거겠지만 post나 truncate post는 무슨 뜻인지 모르겠당\n",
        "# padding: 패딩을 앞에 할 것인가(pre), 뒤로 할 것인가(post). 여기서는 뒤로 하기 위해 post\n",
        "# truncating: 넘치는 글자를 어쩔것인가? 앞을 자를 거면 'pre', 뒤를 자를 거면 'post'\n",
        "# 패딩 달아주는거. 기본 시퀀스 투입하고 최대 길이 정해주고, 패딩 어디 쌓을 건지 정하고(양옆은 없네), 넘치면 어떻게 자를건지 정해줌)\n",
        "\n",
        "print(x_data)\n",
        "print(x_data_len)\n",
        "print(y_data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 6  7  7  4  0  0  0  0  0  0]\n",
            " [ 3  2  4  0  0  0  0  0  0  0]\n",
            " [10  7  8  9  5  0  0  0  0  0]\n",
            " [ 9  7  1  6  7  7  4  0  0  0]]\n",
            "[4, 3, 5, 7]\n",
            "[1, 0, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRnf2otaGAOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "input_dim = len(char2idx)\n",
        "# 사전의 크기\n",
        "output_dim = len(char2idx)\n",
        "# 역시 사전의 크기\n",
        "one_hot = np.eye(len(char2idx))\n",
        "# len(char2idx) * len(char2idx) 크기의 항등행렬을 만든다,\n",
        "# 이 항등행렬을 만든 이유는 각 공간들의 주소값을 주기 위해서인듯 한데..\n",
        "\n",
        "hidden_size = 10\n",
        "num_classes = 2\n",
        "\n",
        "model = Sequential()\n",
        "# Sequential 객체 선언\n",
        "model.add(layers.Embedding(input_dim = input_dim, output_dim = output_dim,\n",
        "                           trainable = False, mask_zero = True, input_length = max_sequence,\n",
        "                           embeddings_initializer = keras.initializers.Constant(one_hot)))\n",
        "# keras 특유의 레이어 쌓기. 근데 인풋과 아웃풋 차원이 이미 정해져있네..\n",
        "# rnn은 원래 한 시퀀스마다 결과를 뱉으니까 가능할 듯\n",
        "# 아니 근데 이거 다대1아니었나? 그럼 마지막에 하나만 나오는거 아닌가?\n",
        "# 그런 시비 보다는 일단 진도 빼자\n",
        "# trainable이 뭐지; one hot을 학습시키지 않는다고 하였다. (1.13에선 변경된 듯)\n",
        "# mask 제로는 연산에서 0이 된 것들을 제외한다고 하였다. mask 방법론이 적용되어 있으면 False로 0도 신경쓰게 하나..\n",
        "# embeddings_initializer = keras.initializers.Constant(one_hot)가 첫 값인데, 이걸 업데이트 하지 않겠다고 봐야하나..\n",
        "# 나중에 다시 보자\n",
        "# 위 코드를 통해 input이 될 층을 쌓은 코드\n",
        "\n",
        "\n",
        "model.add(layers.SimpleRNN(units = hidden_size))\n",
        "# rnn 층 선언\n",
        "model.add(layers.Dense(units = num_classes))\n",
        "# 마지막 층을 도출하는 것으로 유닛 수를 num_class(2 == 좋다, 나쁘다)로 정해 클래시피케이션화 시킨 것"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu196uG5k5jS",
        "colab_type": "code",
        "outputId": "53a06124-fb76-4215-d516-babd4f25fc2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 10, 11)            121       \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 10)                220       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 363\n",
            "Trainable params: 242\n",
            "Non-trainable params: 121\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdziZfAXfmFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#모델 훈련\n",
        "def loss_fn(model, x,y):\n",
        "  return tf.losses.sparse_softmax_cross_entropy(labels = y, logits = model(x))\n",
        "#기존엔 손실함수를 수식으로 썼다면 이번엔 tf가 제공하는 손실함수를 사용, model은 위에 정의된 sequence()에 레이어 추가된 거 말함\n",
        "\n",
        "#옵티마이저 정의\n",
        "lr = 0.01\n",
        "epochs = 30\n",
        "batch_size = 2\n",
        "opt = tf.train.AdamOptimizer(learning_rate = lr)\n",
        "# 아담 옵티마이저(유명)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btNsvruRipnS",
        "colab_type": "code",
        "outputId": "b4aaf855-40b6-4343-c298-b0a4b8814fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#데이터 파이프라인 생성\n",
        "tr_dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
        "tr_dataset = tr_dataset.shuffle(buffer_size = 4)\n",
        "tr_dataset = tr_dataset.batch(batch_size = batch_size)\n",
        "# tr_dataset은 tf.data.Dataset.from_tensor_slices로 구성된다. 일단 여러 텐서를 동시에 올릴 수 있다고 한다.\n",
        "# 큰 건 돌리지 말라고 한다. 한번에 메모리에 올려버리는 거라서\n",
        "# 일단 데이터를 한행 한행 잘라주는 거라고 한다. for 문을 대체하기 위한 걸까?\n",
        "\n",
        "# shuffle은 섞어주는 건데 일정 크기에 데이터 할당해 섞는다고 한다. 그 인수(buffer_size)가 더 커야 한다고 함\n",
        "# batch는 아는 대로 한번에 몇 개씩 넣을건지 정하는 거... 걍 뭐 그런 것이다\n",
        "\n",
        "# 간략하게 tf.data는 딥러닝을 위해 특화된 데이터 함수고 셔플이나 배치 등 데이터 변형 입력 등을 편리하게 해주는 함수로 보임  \n",
        "\n",
        "print(tr_dataset)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((?, 10), (?,)), types: (tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD7MYphfSamT",
        "colab_type": "code",
        "outputId": "304d9d9b-4af0-4d26-ad28-1061efbb7d47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "#훈련\n",
        "tr_loss_hist = []\n",
        "# 그래프 그리기용 히스토리\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  avg_tr_loss = 0\n",
        "  tr_step = 0\n",
        "  # avg_tr_loss 변수 값 선언 후 단계별 손실값을 더하고 스텝별로 나누는 걸로 줄여나감\n",
        "  # tr_step은 epoch랑 같은 것\n",
        "  for x_mb, y_mb in tr_dataset:\n",
        "    with tf.GradientTape() as tape:\n",
        "      tr_loss = loss_fn(model, x = x_mb, y=y_mb)\n",
        "    grads = tape.gradient(target = tr_loss, sources = model.variables)\n",
        "    opt.apply_gradients(grads_and_vars = zip(grads, model.variables))\n",
        "    avg_tr_loss += tr_loss\n",
        "    tr_step += 1\n",
        "  else:\n",
        "    avg_tr_loss /= tr_step\n",
        "    tr_loss_hist.append(avg_tr_loss)\n",
        "  # mb 뜻은 모르지만 걍 변수명으로 가고 손실함수에 앞서 정의된 rnn 모델과 x값 y값을 투입\n",
        "  # grads는 위의 gradienttape를 이용해서 tr_loss를 model.variable 값으로 미분.. \n",
        "  # model.variables는 무엇일까? 모델이 받은 변수일까 생성한 변수일까..\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    print('epoch : {:3}, tr_loss: {:.3f}'.format(epoch + 1, avg_tr_loss))\n",
        "      "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch :   5, tr_loss: 0.380\n",
            "epoch :  10, tr_loss: 0.108\n",
            "epoch :  15, tr_loss: 0.036\n",
            "epoch :  20, tr_loss: 0.016\n",
            "epoch :  25, tr_loss: 0.009\n",
            "epoch :  30, tr_loss: 0.007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQcsPDbdg2kn",
        "colab_type": "code",
        "outputId": "0ebc5e58-537c-4a68-8e19-3f1bd52ec0be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1288
        }
      },
      "source": [
        "print(model.variables)\n",
        "# 모델 변수 자체를 출력해보면 아래와 같다.\n",
        "# 먼저 나오는 행렬은 (11,10) 형태의 행렬이다. 11은 embedingg된 토큰 포함 알파벳 수, 10은 임의로 생성한 히든 사이즈이다.\n",
        "# 두번째로 나온 행렬은 (10,10) 형태의 행렬이다. '\n",
        "# 대충 A<2-1>(kernel:0), Waa(recurrent_kernel:0), ba(bias:0), Wax(embeddings), Way(dens:kernel), by(dense: bias:0) 일 것이다. \n",
        "# 위의 변수들을 조정해서 기울기 값을 최소로 하니까.."
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Variable 'simple_rnn_1/kernel:0' shape=(11, 10) dtype=float32, numpy=\n",
            "array([[-0.21474013, -0.10649455,  0.09093869,  0.2855053 ,  0.3593501 ,\n",
            "        -0.15914246,  0.5344489 , -0.465375  ,  0.1383636 , -0.3194853 ],\n",
            "       [-0.654219  ,  0.44047308, -0.24638326,  0.07234649, -0.13072936,\n",
            "         0.03919677, -0.15056534, -0.29604545, -0.23578192,  0.5514661 ],\n",
            "       [ 0.60564905, -0.2300493 ,  0.1465909 ,  0.38299823, -0.00921232,\n",
            "        -0.11388415, -0.04110971,  0.19311957,  0.05711955, -0.04855765],\n",
            "       [ 0.4798396 ,  0.56426114,  0.24165556,  0.27575386,  0.5888763 ,\n",
            "         0.2687913 , -0.40175882, -0.32942405, -0.01361054,  0.37093264],\n",
            "       [-0.44618788, -0.25219774,  0.09859213,  0.1559581 ,  0.12820792,\n",
            "         0.26009184, -0.30979759,  0.2897816 ,  0.3967795 , -0.00939598],\n",
            "       [-0.28490815, -0.23065552,  0.2862044 ,  0.2936403 , -0.02459965,\n",
            "         0.03557006, -0.19624577,  0.56277865,  0.12202628, -0.16169451],\n",
            "       [ 0.28984466,  0.00817176, -0.51864636, -0.33151144, -0.51619285,\n",
            "         0.29180813,  0.31819683, -0.20608623,  0.2580891 , -0.0113677 ],\n",
            "       [-0.75402504,  0.4837076 , -0.05130032,  0.82892627, -0.57647806,\n",
            "        -0.29293898,  0.15067926,  0.14595133, -0.33566862,  0.35614723],\n",
            "       [ 0.03964885,  0.03937736, -0.02955426,  0.19781543,  0.45728573,\n",
            "         0.25011423,  0.04084032,  0.13314618, -0.07865956, -0.2793825 ],\n",
            "       [ 0.55900246, -0.6528687 , -0.06430601,  0.18462096,  0.4597886 ,\n",
            "        -0.12768197, -0.06894423, -0.28912362,  0.32891572,  0.31463012],\n",
            "       [ 0.4022902 ,  0.03872043,  0.19760998, -0.54651475,  0.508284  ,\n",
            "        -0.28291172, -0.5315004 , -0.18708806,  0.31543478,  0.1793381 ]],\n",
            "      dtype=float32)>, <tf.Variable 'simple_rnn_1/recurrent_kernel:0' shape=(10, 10) dtype=float32, numpy=\n",
            "array([[ 0.14274614, -0.9187649 ,  0.8353478 , -0.44918263, -0.13841312,\n",
            "        -0.41053772, -0.5193759 ,  0.04108873,  0.11138535, -0.34172404],\n",
            "       [-0.2780684 , -0.06178984, -0.20984957, -0.11254244,  0.18622315,\n",
            "         0.41683605,  0.7429573 , -0.46119973,  0.6452489 , -0.04009532],\n",
            "       [ 0.29189542,  0.1365803 , -0.2552013 , -0.37216887, -0.17408367,\n",
            "         0.14669912, -0.7574527 , -0.15530953,  0.3075931 , -0.30076984],\n",
            "       [ 0.006181  ,  0.53358185, -0.13620019,  0.16772686, -0.4699875 ,\n",
            "        -0.22421241,  0.4087037 , -0.06207317,  0.03097319,  0.53659177],\n",
            "       [ 0.21938507, -0.2270297 ,  0.25405002, -0.5593313 ,  0.2384305 ,\n",
            "         0.17817144, -0.41067758,  0.66899645,  0.39836997, -0.05397365],\n",
            "       [ 0.62201804, -0.2962076 ,  0.26539773, -0.3624875 ,  0.16805631,\n",
            "         0.3968335 ,  0.38849387,  0.00845696, -0.3668998 , -0.16019367],\n",
            "       [-0.37947446, -0.3552776 , -0.5454596 ,  0.45157465, -0.46856627,\n",
            "         0.05156286,  0.2514969 ,  0.3988461 , -0.47154412, -0.30203938],\n",
            "       [-0.30083057,  0.6671686 ,  0.33629796,  0.19395594,  0.18689154,\n",
            "         0.31332597, -0.09537429,  0.03875095, -0.14034592, -0.3010828 ],\n",
            "       [ 0.5306483 , -0.04550436,  0.06619522, -0.47356147,  0.15876284,\n",
            "        -0.66814506, -0.24490492,  0.07244526,  0.16718943, -0.74698496],\n",
            "       [ 0.3207718 ,  0.02706001,  0.02169424,  0.2718498 , -0.36349508,\n",
            "         0.28966814,  0.20025127,  0.03371821,  0.41002217, -0.08701861]],\n",
            "      dtype=float32)>, <tf.Variable 'simple_rnn_1/bias:0' shape=(10,) dtype=float32, numpy=\n",
            "array([ 0.12287221, -0.06980048,  0.0272885 , -0.12719694,  0.05791663,\n",
            "       -0.01748757, -0.0455264 ,  0.0611422 ,  0.05984002, -0.05716259],\n",
            "      dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(10, 2) dtype=float32, numpy=\n",
            "array([[ 0.38416934, -0.06769428],\n",
            "       [ 0.10989332,  0.7554585 ],\n",
            "       [ 0.23325682, -0.2412382 ],\n",
            "       [-0.31287414,  0.5166402 ],\n",
            "       [ 0.6760511 ,  0.4066963 ],\n",
            "       [ 0.0124463 ,  0.5743327 ],\n",
            "       [-1.0198966 ,  0.23945293],\n",
            "       [ 0.2587759 , -0.16392736],\n",
            "       [ 0.6184196 ,  0.66437715],\n",
            "       [-0.3384615 ,  0.65780014]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([ 0.05875378, -0.05875378], dtype=float32)>, <tf.Variable 'embedding_1/embeddings:0' shape=(11, 11) dtype=float32, numpy=\n",
            "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLQKKfAtu8Ah",
        "colab_type": "code",
        "outputId": "0aad2258-94dc-42ac-ee46-32258ca5dde5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        " print(tr_loss)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.0077197216, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwvBicRLq4mX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "470b616e-827c-451a-8ea4-9e2119acdbf7"
      },
      "source": [
        "yhat = model.predict(x_data)\n",
        "# 모델의 훈련은 위의 과정을 통해 마무리. 훈련된 가중치는 현재 모델 안에 있는데..\n",
        "# 추후엔 이 모델만 따로 저장하고 다른 곳에선 훈련된 가중치만 쓸 줄 알아야 함\n",
        "yhat = np.argmax(yhat, axis = -1)\n",
        "print(yhat)\n",
        "$\n",
        "print('acc : {:.2%}'.format(np.mean(yhat == y_data)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 1]\n",
            "acc : 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v-nRd1zrKrk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "8d8fe6b6-a01a-4a88-f98d-ccc1519e866e"
      },
      "source": [
        "plt.plot(tr_loss_hist)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f04be7b2e80>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkQD6/3KHD+6d56cZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prlZdUlC5THW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}