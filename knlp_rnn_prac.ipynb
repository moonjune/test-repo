{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "knlp_rnn_prac.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moonjune/test-repo/blob/master/knlp_rnn_prac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSg_lWkdKe60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#참고:  텐서플로와 머신러닝으로 시작하는 자연어 처리\n",
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMPGEMS6KjC9",
        "colab_type": "code",
        "outputId": "27f80a29-7851-4225-9a7c-d48a93e8cbe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "!ls -1ha kaggle.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmjCGyo1KlEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# Permission Warning 이 일어나지 않도록 \n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbh9AMpkKmhy",
        "colab_type": "code",
        "outputId": "7f705997-f503-40dd-ce84-42480af23a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1089
        }
      },
      "source": [
        "!git clone https://github.com/NLP-kr/tensorflow-ml-nlp.git\n",
        "import os\n",
        "os.chdir('/content/tensorflow-ml-nlp')\n",
        "!pip install -r requirements.txt\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'tensorflow-ml-nlp' already exists and is not an empty directory.\n",
            "Requirement already satisfied: tensorflow>=1.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.14.0rc1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.24.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.0.3)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.0.1)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (3.2.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (3.6.0)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (0.5.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (0.90)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (4.28.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.16.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.33.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.14.0rc1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->-r requirements.txt (line 2)) (0.21.2)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->-r requirements.txt (line 5)) (4.6.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->-r requirements.txt (line 6)) (0.5.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud->-r requirements.txt (line 7)) (4.3.0)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r requirements.txt (line 10)) (1.8.4)\n",
            "Requirement already satisfied: JPype1>=0.5.7 in /usr/local/lib/python3.6/dist-packages (from konlpy->-r requirements.txt (line 11)) (0.6.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.10->-r requirements.txt (line 1)) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.10->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow>=1.10->-r requirements.txt (line 1)) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow>=1.10->-r requirements.txt (line 1)) (2.8.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 2)) (0.13.2)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->wordcloud->-r requirements.txt (line 7)) (0.46)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (1.9.167)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (2.21.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (2.49.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.167 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (1.12.167)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (1.24.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.167->boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (0.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7riVAncoKpOr",
        "colab_type": "code",
        "outputId": "de641f9b-3767-41fd-9a62-463f197a31b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "!mkdir -p /content/kaggle_bag_of_word\n",
        "os.chdir('/content/kaggle_bag_of_word')\n",
        "!kaggle competitions download -c word2vec-nlp-tutorial"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sampleSubmission.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "unlabeledTrainData.tsv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "testData.tsv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "labeledTrainData.tsv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqGELMqqKr4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "\n",
        "DATA_IN_PATH = '/content/kaggle_bag_of_word/'\n",
        "\n",
        "file_list = ['labeledTrainData.tsv.zip','unlabeledTrainData.tsv.zip', 'testData.tsv.zip']\n",
        "\n",
        "for file in file_list:\n",
        "  zipRef = zipfile.ZipFile(DATA_IN_PATH + file, 'r')\n",
        "  zipRef.extractall(DATA_IN_PATH)\n",
        "  zipRef.close()\n",
        "             "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXcliHH4K3J_",
        "colab_type": "code",
        "outputId": "c6e88555-f9f3-4847-fc41-5d77d2f206ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline # 그래프를 주피터 노트북에서 바로 그리도록 함"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: unrecognized arguments: # 그래프를 주피터 노트북에서 바로 그리도록 함\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDt74geEK3Zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv(DATA_IN_PATH+\"labeledTrainData.tsv\", header = 0, delimiter = \"\\t\", quoting = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpyTN_3GK6yY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_word_counts = train_data['review'].apply(lambda x:len(x.split(' ')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAmPqCa6LOh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyeo4octLUhp",
        "colab_type": "code",
        "outputId": "0d14486d-d69a-42e0-c4dc-ca536654cefc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qLmZKhKLVnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_IN_PATH = '/content/kaggle_bag_of_word/'\n",
        "train_data = pd.read_csv(DATA_IN_PATH + 'labeledTrainData.tsv',header = 0, delimiter = '\\t', quoting = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAGHjshKNHMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(review, remove_stopwords = False):\n",
        "  review_text = BeautifulSoup(review, \"html5lib\").get_text()\n",
        "  review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
        "  words = review_text.lower().split()\n",
        "  if remove_stopwords:\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    words = [w for w in words if not w in stops]\n",
        "    clean_review = \" \".join(words)\n",
        "  else:\n",
        "    clean_review = \" \".join(words)\n",
        "    \n",
        "  return clean_review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-L4NV5jNIdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_train_reviews = []\n",
        "for review in train_data['review']:\n",
        "  clean_train_reviews.append(preprocessing(review, remove_stopwords = True))\n",
        "  \n",
        "clean_train_df = pd.DataFrame({'review':clean_train_reviews, 'sentiment': train_data['sentiment']})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EONI2wYpNLhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_train_reviews)\n",
        "text_sequences = tokenizer.texts_to_sequences(clean_train_reviews)\n",
        "\n",
        "word_vocab = tokenizer.word_index\n",
        "\n",
        "data_configs = {}\n",
        "\n",
        "data_configs['vocab'] = word_vocab\n",
        "data_configs['vocab_size'] = len(word_vocab)+1\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 174\n",
        "train_inputs = pad_sequences(text_sequences, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post')\n",
        "train_labels = np.array(train_data['sentiment'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXlJKDbSNWNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_IN_PATH = '/content/kaggle_bag_of_word/'\n",
        "TRAIN_INPUT_DATA = 'train_input.npy'\n",
        "TRAIN_LABEL_DATA = 'train_label.npy'\n",
        "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
        "DATA_CONFIGS = 'data_configs.json'\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists(DATA_IN_PATH):\n",
        "  os.makedir(DATA_IN_PATH)\n",
        "\n",
        "np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)\n",
        "np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)\n",
        "\n",
        "clean_train_df.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index = False)\n",
        "\n",
        "json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'), ensure_ascii=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kjypqa3O54u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv(DATA_IN_PATH + \"testData.tsv\", header = 0, delimiter = \"\\t\", quoting = 3)\n",
        "\n",
        "clean_test_reviews = []\n",
        "\n",
        "for review in test_data['review']:\n",
        "  clean_test_reviews.append(preprocessing(review, remove_stopwords = True))\n",
        "clean_test_df =  pd.DataFrame({'review': clean_test_reviews, 'id': test_data['id']})\n",
        "test_id = np.array(test_data['id'])\n",
        "\n",
        "tokenizer.fit_on_texts(clean_test_reviews)\n",
        "text_sequences = tokenizer.texts_to_sequences(clean_test_reviews)\n",
        "test_inputs = pad_sequences(text_sequences, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv1XsOPqPDl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_IN_PATH = '/content/kaggle_bag_of_word/'\n",
        "DATA_OUT_PATH = '/content/kaggle_bag_of_word/data_out/'\n",
        "\n",
        "INPUT_TRAIN_DATA_FILE_NAME = 'train_input.npy'\n",
        "LABEL_TRAIN_DATA_FILE_NAME = 'train_label.npy'\n",
        "DATA_CONFIGS_FILE_NAME = 'data_configs.json'\n",
        "\n",
        "train_input = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA_FILE_NAME, 'rb'))\n",
        "train_label = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA_FILE_NAME, 'rb'))\n",
        "\n",
        "prepro_configs = None\n",
        "\n",
        "with open(DATA_IN_PATH + DATA_CONFIGS_FILE_NAME, 'r') as f:\n",
        "  prepro_configs = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2wsBTIXP53W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TEST_SPLIT = 0.2\n",
        "RANDOM_SEED = 13371447\n",
        "\n",
        "input_train, input_eval, label_train, lavel_eval = train_test_split(train_input,\n",
        "                                                                    train_label,\n",
        "                                                                    test_size = TEST_SPLIT,\n",
        "                                                                    random_state = RANDOM_SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0aBcu_7XEr_",
        "colab_type": "code",
        "outputId": "a417bdd5-a78d-4871-87ee-f6370747da22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "def mapping_fn(X, Y):\n",
        "  inputs, labels = {'x' : X}, Y\n",
        "  return inputs, labels\n",
        "\n",
        "def train_input_fn():\n",
        "  dataset = tf.data.Dataset.from_tensor_slice((input_train, label_train))\n",
        "  dataset = dataset.shuffle(buffer_size = 1000)\n",
        "  dataset = dataset.map(mapping_fn)\n",
        "  dataset = dataset.repeat(count = NUM_EPOCHS)\n",
        "  iterator = dataset.make_one_shot_iterator()\n",
        "  \n",
        "  return iterator.get_next()\n",
        "\n",
        "def eval_input_fn():\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((input_test, label_test))\n",
        "  dataset = dataset.map(mapping_fn)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  iterator = dataset.make_one_shot_iterator()\n",
        "  \n",
        "  return iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 174)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xNdYbUJb5W1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = prepro_configs['vocab_size']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrvHdXDnU2Mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORD_EMBEDDING_DIM = 1000\n",
        "HIDDEN_STATE_DIM = 150\n",
        "DENSE_FEATURE_DIM = 150\n",
        "\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc3ceoTEVAd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode):\n",
        "  TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
        "  EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
        "  PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
        "  \n",
        "  embedding_layer = tf.keras.layers.Embedding(VOCAB_SIZE,\n",
        "                                             WORD_EMBEDDING_DIM)(features['x'])\n",
        "  \n",
        "  embedding_layer = tf.keras.layers.Dropout(0.2)(embedding_layer)\n",
        "  \n",
        "  rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [HIDDEN_STATE_DIM, HIDDEN_STATE_DIM]]\n",
        "  multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
        "  \n",
        "  outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}