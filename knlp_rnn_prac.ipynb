{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "knlp_rnn_prac.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moonjune/test-repo/blob/master/knlp_rnn_prac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSg_lWkdKe60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#참고:  텐서플로와 머신러닝으로 시작하는 자연어 처리\n",
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMPGEMS6KjC9",
        "colab_type": "code",
        "outputId": "10bff544-fed3-4b53-ad94-8ab4489c016a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "!ls -1ha kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmjCGyo1KlEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# Permission Warning 이 일어나지 않도록 \n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbh9AMpkKmhy",
        "colab_type": "code",
        "outputId": "b4d5430b-a9af-4559-d010-6ec8be587462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1397
        }
      },
      "source": [
        "!git clone https://github.com/NLP-kr/tensorflow-ml-nlp.git\n",
        "import os\n",
        "os.chdir('/content/tensorflow-ml-nlp')\n",
        "!pip install -r requirements.txt\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tensorflow-ml-nlp'...\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 845 (delta 32), reused 27 (delta 15), pack-reused 778\u001b[K\n",
            "Receiving objects: 100% (845/845), 160.14 MiB | 26.27 MiB/s, done.\n",
            "Resolving deltas: 100% (499/499), done.\n",
            "Checking out files: 100% (100/100), done.\n",
            "Requirement already satisfied: tensorflow>=1.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.14.0rc1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.24.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.0.3)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.0.1)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (3.2.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (3.6.0)\n",
            "Collecting konlpy (from -r requirements.txt (line 11))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/3d/4e983cd98d87b50b2ab0387d73fa946f745aa8164e8888a714d5129f9765/konlpy-0.5.1-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (0.90)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (4.28.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.14.0rc1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.33.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->-r requirements.txt (line 1)) (1.16.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->-r requirements.txt (line 2)) (0.21.2)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->-r requirements.txt (line 5)) (4.6.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->-r requirements.txt (line 6)) (0.5.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud->-r requirements.txt (line 7)) (4.3.0)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r requirements.txt (line 10)) (1.8.4)\n",
            "Collecting JPype1>=0.5.7 (from konlpy->-r requirements.txt (line 11))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/4b/60a3e63d51714d4d7ef1b1efdf84315d118a0a80a5b085bb52a7e2428cdc/JPype1-0.6.3.tar.gz (168kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow>=1.10->-r requirements.txt (line 1)) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.10->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.10->-r requirements.txt (line 1)) (0.15.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow>=1.10->-r requirements.txt (line 1)) (2.8.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 2)) (0.13.2)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->wordcloud->-r requirements.txt (line 7)) (0.46)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (1.9.167)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.167 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (1.12.167)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (0.2.1)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.167->boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 10)) (0.14)\n",
            "Building wheels for collected packages: JPype1\n",
            "  Building wheel for JPype1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/2b/e8/c0b818ac4b3d35104d35e48cdc7afe27fc06ea277feed2831a\n",
            "Successfully built JPype1\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-0.6.3 konlpy-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7riVAncoKpOr",
        "colab_type": "code",
        "outputId": "aa30beb1-7eba-44ba-a542-c274b48de35c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "!mkdir -p /content/kaggle_bag_of_word\n",
        "os.chdir('/content/kaggle_bag_of_word')\n",
        "!kaggle competitions download -c word2vec-nlp-tutorial"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sampleSubmission.csv to /content/kaggle_bag_of_word\n",
            "\r  0% 0.00/276k [00:00<?, ?B/s]\n",
            "100% 276k/276k [00:00<00:00, 37.8MB/s]\n",
            "Downloading unlabeledTrainData.tsv.zip to /content/kaggle_bag_of_word\n",
            " 35% 9.00M/26.0M [00:00<00:00, 20.8MB/s]\n",
            "100% 26.0M/26.0M [00:00<00:00, 44.3MB/s]\n",
            "Downloading testData.tsv.zip to /content/kaggle_bag_of_word\n",
            " 95% 12.0M/12.6M [00:00<00:00, 39.7MB/s]\n",
            "100% 12.6M/12.6M [00:00<00:00, 50.1MB/s]\n",
            "Downloading labeledTrainData.tsv.zip to /content/kaggle_bag_of_word\n",
            " 69% 9.00M/13.0M [00:00<00:00, 35.9MB/s]\n",
            "100% 13.0M/13.0M [00:00<00:00, 43.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqGELMqqKr4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "\n",
        "DATA_IN_PATH = '/content/kaggle_bag_of_word/'\n",
        "\n",
        "file_list = ['labeledTrainData.tsv.zip','unlabeledTrainData.tsv.zip', 'testData.tsv.zip']\n",
        "\n",
        "for file in file_list:\n",
        "  zipRef = zipfile.ZipFile(DATA_IN_PATH + file, 'r')\n",
        "  zipRef.extractall(DATA_IN_PATH)\n",
        "  zipRef.close()\n",
        "             "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXcliHH4K3J_",
        "colab_type": "code",
        "outputId": "54ed1d11-18d2-4729-f75f-1547ae3de89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline # 그래프를 주피터 노트북에서 바로 그리도록 함"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: unrecognized arguments: # 그래프를 주피터 노트북에서 바로 그리도록 함\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDt74geEK3Zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv(DATA_IN_PATH+\"labeledTrainData.tsv\", header = 0, delimiter = \"\\t\", quoting = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpyTN_3GK6yY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_word_counts = train_data['review'].apply(lambda x:len(x.split(' ')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAmPqCa6LOh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyeo4octLUhp",
        "colab_type": "code",
        "outputId": "0b286080-601b-425b-d977-1ebcd9af93d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qLmZKhKLVnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_IN_PATH = '/content/kaggle_bag_of_word/'\n",
        "train_data = pd.read_csv(DATA_IN_PATH + 'labeledTrainData.tsv',header = 0, delimiter = '\\t', quoting = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAGHjshKNHMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(review, remove_stopwords = False):\n",
        "  review_text = BeautifulSoup(review, \"html5lib\").get_text()\n",
        "  review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
        "  words = review_text.lower().split()\n",
        "  if remove_stopwords:\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    words = [w for w in words if not w in stops]\n",
        "    clean_review = \" \".join(words)\n",
        "  else:\n",
        "    clean_review = \" \".join(words)\n",
        "    \n",
        "  return clean_review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-L4NV5jNIdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_train_reviews = []\n",
        "for review in train_data['review']:\n",
        "  clean_train_reviews.append(preprocessing(review, remove_stopwords = True))\n",
        "  \n",
        "clean_train_df = pd.DataFrame({'review':clean_train_reviews, 'sentiment': train_data['sentiment']})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EONI2wYpNLhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_train_reviews)\n",
        "text_sequences = tokenizer.texts_to_sequences(clean_train_reviews)\n",
        "\n",
        "word_vocab = tokenizer.word_index\n",
        "\n",
        "data_configs = {}\n",
        "\n",
        "data_configs['vocab'] = word_vocab\n",
        "data_configs['vocab_size'] = len(word_vocab)+1\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 174\n",
        "train_inputs = pad_sequences(text_sequences, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post')\n",
        "train_labels = np.array(train_data['sentiment'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXlJKDbSNWNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_IN_PATH = '/content/kaggle_bag_of_word/'\n",
        "TRAIN_INPUT_DATA = 'train_input.npy'\n",
        "TRAIN_LABEL_DATA = 'train_label.npy'\n",
        "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
        "DATA_CONFIGS = 'data_configs.json'\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists(DATA_IN_PATH):\n",
        "  os.makedir(DATA_IN_PATH)\n",
        "\n",
        "np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)\n",
        "np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)\n",
        "\n",
        "clean_train_df.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index = False)\n",
        "\n",
        "json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'), ensure_ascii=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kjypqa3O54u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv(DATA_IN_PATH + \"testData.tsv\", header = 0, delimiter = \"\\t\", quoting = 3)\n",
        "\n",
        "clean_test_reviews = []\n",
        "\n",
        "for review in test_data['review']:\n",
        "  clean_test_reviews.append(preprocessing(review, remove_stopwords = True))\n",
        "clean_test_df =  pd.DataFrame({'review': clean_test_reviews, 'id': test_data['id']})\n",
        "test_id = np.array(test_data['id'])\n",
        "\n",
        "tokenizer.fit_on_texts(clean_test_reviews)\n",
        "text_sequences = tokenizer.texts_to_sequences(clean_test_reviews)\n",
        "test_inputs = pad_sequences(text_sequences, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv1XsOPqPDl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_IN_PATH = '/content/kaggle_bag_of_word/'\n",
        "DATA_OUT_PATH = '/content/kaggle_bag_of_word/data_out/'\n",
        "\n",
        "INPUT_TRAIN_DATA_FILE_NAME = 'train_input.npy'\n",
        "LABEL_TRAIN_DATA_FILE_NAME = 'train_label.npy'\n",
        "DATA_CONFIGS_FILE_NAME = 'data_configs.json'\n",
        "\n",
        "train_input = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA_FILE_NAME, 'rb'))\n",
        "train_label = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA_FILE_NAME, 'rb'))\n",
        "\n",
        "prepro_configs = None\n",
        "\n",
        "with open(DATA_IN_PATH + DATA_CONFIGS_FILE_NAME, 'r') as f:\n",
        "  prepro_configs = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2wsBTIXP53W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TEST_SPLIT = 0.2\n",
        "RANDOM_SEED = 13371447\n",
        "\n",
        "input_train, input_eval, label_train, lavel_eval = train_test_split(train_input,\n",
        "                                                                    train_label,\n",
        "                                                                    test_size = TEST_SPLIT,\n",
        "                                                                    random_state = RANDOM_SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0aBcu_7XEr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "def mapping_fn(X, Y):\n",
        "  inputs, labels = {'x' : X}, Y\n",
        "  return inputs, labels\n",
        "\n",
        "def train_input_fn():\n",
        "  dataset = tf.data.Dataset.from_tensor_slice((input_train, label_train))\n",
        "  dataset = dataset.shuffle(buffer_size = 1000)\n",
        "  dataset = dataset.map(mapping_fn)\n",
        "  dataset = dataset.repeat(count = NUM_EPOCHS)\n",
        "  iterator = dataset.make_one_shot_iterator()\n",
        "  \n",
        "  return iterator.get_next()\n",
        "\n",
        "def eval_input_fn():\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((input_test, label_test))\n",
        "  dataset = dataset.map(mapping_fn)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  iterator = dataset.make_one_shot_iterator()\n",
        "  \n",
        "  return iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xNdYbUJb5W1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = prepro_configs['vocab_size']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrvHdXDnU2Mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORD_EMBEDDING_DIM = 1000\n",
        "HIDDEN_STATE_DIM = 150\n",
        "DENSE_FEATURE_DIM = 150\n",
        "\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc3ceoTEVAd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode):\n",
        "  TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
        "  EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
        "  PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
        "  \n",
        "  embedding_layer = tf.keras.layers.Embedding(VOCAB_SIZE,\n",
        "                                             WORD_EMBEDDING_DIM)(features['x'])\n",
        "  \n",
        "  embedding_layer = tf.keras.layers.Dropout(0.2)(embedding_layer)\n",
        "  \n",
        "  rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [HIDDEN_STATE_DIM, HIDDEN_STATE_DIM]]\n",
        "  multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
        "  \n",
        "  outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,inputs = embedding_layer, dtype = tf.float32)\n",
        "  hidden_layer = tf.keras.layers.Dense(DENSE_FEATURE_DIM, activation = tf.nn.tanh)(outputs[:,-1,:])\n",
        "  hidden_layer = tf.keras.layers.Dropout(0.2)(hidden_layer)\n",
        "  logits = tf.keras.layers.Dense(1)(hidden_layer)\n",
        "  logits = tf.squeeze(logits, axis = -1)\n",
        "  \n",
        "  sigmoid_logits = tf.nn.sigmoid(logits)\n",
        "  \n",
        "  if PREDICT:\n",
        "    predictions = {'sentiment': sigmoid_logits}\n",
        "    \n",
        "    return tf.estimator.EstimatorSpec(mode = mode,\n",
        "                                      predictions = predictions)\n",
        "  \n",
        "  loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
        "  \n",
        "  if EVAL:\n",
        "    accuracy = tf.metrics.accuracy(labels, tf.round(sigmoid_logits))\n",
        "    eval_metric_ops = {'acc': accuracy}\n",
        "    \n",
        "    return tf.estimator.EstimatorSpec(mode, loss = loss,\n",
        "                                      eval_metric_ops = eval_metric_ops)\n",
        "  \n",
        "  if TRAIN:\n",
        "    global_step = tf.train.get_global_step()\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step)\n",
        "    \n",
        "    return tf.estimator.EsimatorSpec(mode = mode,\n",
        "                                     train_op = train_op,\n",
        "                                     loss = loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzVT10puXEOa",
        "colab_type": "code",
        "outputId": "c213a721-97e9-4968-9603-55f5420433b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "accuracy = tf.metrics.accuracy(labels, tf.round(sigmoid_logits))\n",
        "eval_metric_ops = {'acc':accuracy}\n",
        "\n",
        "return tf.estimator.EstimatorSpec(mode, loss = loss, eval_metric_ops = eval_metric_ops)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-863ef74670ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval_metric_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimatorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_metric_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-kDoffhBu7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.zeros((5,5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw__i9C-B1g9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}